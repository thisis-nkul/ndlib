{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38264bite84bebfac701438ebc1e8670f68f7992",
   "display_name": "Python 3.8.2 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is implemented and I'm @thisis_nkul(you can follow me on instagram :P)\n",
    "\n",
    "import ndlib.nn as nn\n",
    "import ndlib.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNN:\n",
    "    def __init__(self, layer_num, layer_units=[1, 1], initializations = [], activations = []):\n",
    "        # layer_num: total number of layers excluding input layer\n",
    "        # layer_units[i]: number of hidden units in layer i input layer is layer0\n",
    "        # initializations: define initialization, 0 in array means default, i.e., HE initialization\n",
    "        # activations: activations[i] is activation of (i+1)th layer, if an element is 0: no activation                          applied\n",
    "\n",
    "\n",
    "        self.layer_num = layer_num\n",
    "        self.layer_units = layer_units\n",
    "        self.initializations = initializations\n",
    "        self.activations = activations\n",
    "        self.output = 0\n",
    "\n",
    "        self.layers = []                #Collection of layers for our NN\n",
    "        self.layers.append(nn.InputHolder(0))         #layers[0] is input layer\n",
    "\n",
    "        #some default inits for activations and weight_initialization\n",
    "        if len(self.initializations) == 0:\n",
    "            for i in range(layer_num+1):\n",
    "                self.initializations.append(0)\n",
    "        \n",
    "        #TODO: implement activations cheching and replace activation[i]=0 with 'linear' or relu\n",
    "        '''\n",
    "        if len(self.activations) == 0:\n",
    "            for i in range(layer_num):\n",
    "                self.activations.append('identity')\n",
    "\n",
    "        for j in range(len(self.activations)):\n",
    "            if self.activations[j] == 0:\n",
    "                self.activations[j] = 'identity'\n",
    "        '''\n",
    "\n",
    "        #checks\n",
    "        if len(self.layer_units) != self.layer_num+1:\n",
    "            raise Exception('Hidden Units are not correctly specified for each layer!')\n",
    "\n",
    "        if  len(activations) != layer_num:\n",
    "            raise Exception('Activations are not properly defined for layers')\n",
    "\n",
    "\n",
    "        #creting layers\n",
    "        for i in range(layer_num):\n",
    "            if(initializations[i]!=None and initializations[i]!=0):\n",
    "                self.layers.append(nn.Linear(layer_units[i],\n",
    "                                        layer_units[i+1],\n",
    "                                        initialization=initializations[i], activation=activations[i]))\n",
    "\n",
    "            else:\n",
    "                self.layers.append(nn.Linear(layer_units[i],\n",
    "                                        layer_units[i+1], activation=activations[i]))\n",
    "\n",
    "    def forward(self, inp):\n",
    "        #inp: input vector/matrix X\n",
    "        self.layers[0].Z = inp               #totally optional. If this consumes more memory than directly                                             #using input, remove this and use inp directly\n",
    "                                           #but this might help in backprop, idk. No ig it won't.\n",
    "\n",
    "        #self.output = inp                   #initialized this way bcz the uppr init might take some more\n",
    "                                            #memory\n",
    "\n",
    "        for j in range(self.layer_num):\n",
    "            self.output = self.layers[j+1](self.layers[j].Z)\n",
    "\n",
    "        return self.output\n",
    "\n",
    "    __call__ = forward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "metadata": {},
     "execution_count": 32
    }
   ],
   "source": [
    "mdn = SimpleNN(3, [10, 8, 4, 2], activations=['relu', 'relu', 'softmax'])\n",
    "x = np.random.randn(10, 1)\n",
    "mdn.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[0.600992],\n",
       "       [0.399008]])"
      ]
     },
     "metadata": {},
     "execution_count": 33
    }
   ],
   "source": [
    "mdn(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdn1 = nn.SimpleNN(3, [10, 8, 4, 2], activations=['relu', 'relu', 'softmax'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[0.45521048, 0.45521048, 0.45521048, 0.45521048, 0.45521048,\n",
       "        0.45521048, 0.45521048, 0.45521048, 0.45521048, 0.45521048],\n",
       "       [0.54478952, 0.54478952, 0.54478952, 0.54478952, 0.54478952,\n",
       "        0.54478952, 0.54478952, 0.54478952, 0.54478952, 0.54478952]])"
      ]
     },
     "metadata": {},
     "execution_count": 35
    }
   ],
   "source": [
    "mdn1(np.random.randn(10, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[4.49531874],\n",
       "       [4.0857186 ]])"
      ]
     },
     "metadata": {},
     "execution_count": 36
    }
   ],
   "source": [
    "mdn.layers[3].Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'softmax'"
      ]
     },
     "metadata": {},
     "execution_count": 38
    }
   ],
   "source": [
    "mdn.layers[3].activation"
   ]
  }
 ]
}